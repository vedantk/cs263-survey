\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{indentfirst}
\usepackage[margin=1in]{geometry}
\usepackage[doublespacing]{setspace}

\title{A Survey of Shape Analysis Techniques}
\author{Vedant Kumar, \texttt{vsk@berkeley.edu}}

\begin{document}
\maketitle

\section{Introduction}

Shape analysis techniques statically determine whether the contents of a
program's memory can satisfy a set of structural invariants. The basic
problems in shape analysis are (1) to decompose a program into a set of
locations, (2) to conservatively determine what these locations point to,
and (3) to use this `points-to' information to uncover the shape of
in-memory structures. In short, the goal of shape analysis is to answer
questions about a program's memory-usage patterns without actually running
it.

The ability to answer these questions is powerful and broadly applicable.
For example, in the field of compile-time optimization the task of
automatically transforming a program into parallel fragments requires shape
analysis to flag conflicting memory operations [LH88]. The graph structures
used in several shape analysis methods (e.g [HPR89], [LH88], and [SRW96])
are amenable to solving not just this problem, but other related
dependency-tracking problems which arise in optimization. In the field of
program verification, shape analysis can be used to check if variables
satisfy sophisticated invariants such as `is-a-list?', `is-a-tree?', and
`contains-cycles?' [SRW96]. Enforcing these shape invariants statically is
useful. 

This paper is organized in a top-down fashion. So far in Section 1 we have
characterized shape analysis, stated the basic problems in the field, and
motivated further study. Section 2 delves into the fundamental approaches to
solving shape analysis problems. Section 3 discusses the main achievements
in the field. Section 4 presents the challenges remaining in the field.
Section 5 concludes.

\section{Solving shape analysis problems}

The wealth of research in this field has lead to multiple different
approaches to solving the shape analysis problem. Instead of describing each
approach individually, we discuss their commonalities and make
generalizations whenever appropriate. In this section, we focus on the
following recurring motifs: specialized program representation, abstract
interpretation, and the construction of a shape graph via lattice
operations.

\subsection{Program representation}

The choice of program representation affects the scope and complexity of
analysis algorithms. In the literature there are two common classes of
representations. The first is the Lisp-y \texttt{cons}-cell model espoused
by [JM82], [LH88], and [SRW96]. The other is the C-like model employed by
[Steen95], [AG01], and [KS13]. The Lisp model focuses on the challenges
posed by loads, stores, reference passing, and recursive data structures.
The C model is similar, but considers pointer dereferencing and a slew of
unsafe memory operations as well. While this model enables analysis of a
larger class of programs, some authors eschew it in order to focus on
fundamentals.  Other program preprocessing steps exist, such as the
injection of `kill' instructions before all assignments (as seen in
[SRW96]). This simple trick leads to simplified data-flow equations as
compared to those in [LH88]. Such `frontend-level' differences are useful
and drive diversity in the literature, but are ultimately engineering
concerns.

\subsection{Abstract interpretation}

Abstract interpretation is akin to normal program interpretation (i.e
execution), but with a specialized operational semantics. The key ideas are
(1) to replace concrete program values with abstract values from a lattice,
and (2) to then simulate the resulting program using the new operational
semantics until a fixed-point is reached. Shape analysis algorithms utilize
abstract interpretation to compute dataflow relations between the various
locations in a program, resulting in a model of the in-memory structures of
a program.

The first step in abstract interpretation is to safely replace concrete
program values with abstract ones. In [JM82] \footnote{We focus on
definitions from [JM82] and treat them as representative to guide our
discussion throughout this subsection.}, the authors specify a
concrete-value lattice $A$, an accompanying abstract approximation lattice
$A'$, and a pair of functions which translate between the two lattices: $abs
: A \rightarrow A'$ and $conc : A' \rightarrow A$. They then state an
important \textit{safe approximation criterion}: given an $n$-ary concrete
operation $\varphi : A^n \rightarrow A$, an approximation operation
$\varphi' : (A')^n \rightarrow A'$ is safe if for all $a_1, ..., a_n$:
$\{abs(\varphi(a_1, ..., a_n)) \mid \forall i.  a_i \in conc(a'_i)\}
\subseteq \varphi'(a'_1, ..., a'_n)$. The intuition behind this requirement
is that the abstract $n$-ary operation must contain all values that result
from abstracting any feasible concrete version of the abstract operation.
This is a general criterion which sound abstraction function must comply
with (e.g the $combine_{ij}$ rule for merging abstract environments in the
\texttt{escape} and \texttt{return} operations rely on this property).

The second step in abstract interpretation is to define concrete and
abstract versions of the program state.  The concrete state of a program is
an element $\sigma \in Q \times A \times L$, where $Q$ is the set of
control-flow edges and $L$ is the set of locations. The abstract state
replaces the location set with the token set $T$. In addition, each state is
equipped with a partial \textit{retrieval function} ($\tau : T \rightarrow
A' \times 2^{T \times T}$), which either maps tokens to atomic abstract
values, or to two more tokens. These $\tau$-functions allow the abstract
representation to model two program data types: atomic values and binary
lists.

The third step in abstract interpretation is to define the semantic action
of each program construct. This is a mechanical step, so we provide more
detail in the next subsection and curtail the discussion here. It suffices
to say that coupled with an equivalence relation abstract states, these
semantics allow computation of the subset $S' \subseteq \Delta$ of reachable
abstract states in a program. The abstract interpretation is actually
effected by a simulation function $f : \Delta \rightarrow \Delta$. The
interpretation terminates when a least fixed point is found. The set
$\Delta$ is ordered by subset inclusion: existence of the least fixed point
follows from the Tarski-Knaster theorem provided $f$ is monotone and that
there are no infinitely ascending chains in $\Delta$. This simulation can
capture accurate descriptions of recursive data structures, perform
interprocedural analysis, and trade speed for precision by adjusting its
token sets. It is a flexible and foundational idea.

\subsection{Shape graphs}

Shape graphs are an alternate form of abstract state representation
introduced in [SRW96]. We focus on shape graphs even though they are not
ubiquitous because they have been used to drive key advances in the field.
Shape graphs are also conceptually similar to the alias graph in [LH88] and
(to some extent) to the retrieval functions of [JM82], so our discussion
will not be too idiosyncratic. We define shape graphs, examine how they are
constructed, discuss some interesting properties, and compare them to other
abstract state representations.

This structure is formally defined in [SRW96] as a
finite digraph $\langle E_v, E_s \rangle$ consisting of \textit{shape
nodes}, \textit{variable edges}, and \textit{selector edges}. Variable edges
of the form $[x, n]$ reside in $E_v$, where $x$ is a pointer variable and
$n$ is a shape node. Selector edges of the form $[s, sel, t]$ reside in
$E_s$, where $sel$ is a selector and $s$ and $t$ are shape nodes. The sets
$E_v$ and $E_s$ fully describe the graph structure. 

$\mathcal{DSG}$



Interesting properties of shape graphs;
\begin{enumerate}[1.]
    \item Shape nodes are named using a possibly empty set of variables. A
        shape node is named by the variables that all point to the same
        run-time location at the selected program point. This naming scheme
        is fluid, and does not force shape nodes to irreversibly partion
        memory. As a direct consequence, multiple shape nodes may exist at
        a program point. Further, collections of shape nodes can be
        summarized into a single node, and later may be un-summarized if
        necessary.

    \item The concrete semantics generate deterministic shape graphs.
    \item 
\end{enumerate}

Details I'm fuzzy on, which could be important;
\begin{enumerate}[1.]
    \item Concrete vs. collecting vs. abstract semantics.
    \item Lattice structure used to revise shape graphs.
    \item Handling interprocedural flows, context-sensitivity?
    \item Handling destructive pointer updates?
\end{enumerate}

\section{Main achivements}

Fastest?
Most space-efficient?
Best complexity?
Most precise?

Mention invariants Sagiv/Reps are able to capture.
Discussion of Steensgaard here.
Discussion of context sensitive/insensitive.

\section{Current limitations}

The perfect shape analysis algorithm produces a deterministic shape graph
that accurately describes the contents of memory at any selected point in
time. Three program features make such an algorithm infeasible:
non-determinism, destructive updating, and recursion. 

\begin{enumerate}[1.]
    \item NP-hardness of approximation (see Larus/Hilfinger).
    \item Concerns about tractability.
    \item Usually deals with heap-allocated storage, ignores the stack, and
        other types of (potentially volatile) memory.
    \item NULL pointer dereferences.
    \item Dealing with unsafe deallocations (\texttt{free()}).
    \item Difficulty of implementation.
    \item Constructing a conservative approximation quickly.
\end{enumerate}

\section{Conclusion}

Future work;
\begin{enumerate}[1.]
    \item Designing parallel shape analysis algorithms.
    \item Determine the best way to trade precision for speed.
    \item Overcoming as many of the limitations mentioned above as possible.
\end{enumerate}

Shape analyses determine the structure of dynamically-updated storage
statically.

Most immediately, it lets us answer questions about how a program organizes
memory without running it. For example, we may specify a set of invariants
for objects in memory and determine whether or not these invariants are ever
violated (i.e we may model-check data structures). Shape analysis enables
invariants which constrain memory accesses, allowing us to answer relevant
questions about memory sharing and reachability in programs.

\section{Bibliography}

\begin{enumerate}[1.]
    \item U. A{\ss}mann and M. Weinhardt. Interprocedural Heap Analysis for
        Parallelizing Imperative Programs. In \textit{IEEE, Programming
        Models for Massively Parallel Computers}, pages 74-82, 1993.
    \item D. Atkinson and W. Griswold. Effective Whole-Program Analysis in
        the Presence of Pointers. In \textit{SIGSOFT '98/FSE-6 Proceedings
        of the 6th ACM SIGSOFT'}, pages 46-55, 1998.
    \item D. Atkinson and W. Griswold. Implementation Techniques for
        Efficient Data-Flow Analysis of Large Programs. In
        \textit{Proceedings of the International Conference on Software
        Maintenance}, pages 52-61, 2001.
    \item S. Horwitz, P. Pfeiffer, and T. Reps. Dependence Analysis for
        Pointer Variables. In \textit{Proceedings of the ACM SIGPLAN 
        Conference on Programming Language Design and Implementation}, pages
        28-40, 1989.
    \item N. Jones and S. Muchnick. A Flexible Approach to Interprocedural
        Data Flow Analysis and Programs with Recursive Data Structures. In
        \textit{ACM Symposium on Principles of Programming Languages}, pages
        66-74, 1982.
    \item G. Kastrinis and Y. Smaragdakis. Hybrid Context-Sensitivity for
        Points-To Analysis. In \textit{Proceedings of the 34th ACM SIGPLAN
        conference on Programming Language Design and Implementation}, pages
        423-434, 2013.
    \item J. Larus and P. Hilfinger. Detecting Conflicts Between Structure
        Accesses. In \textit{SIGPLAN Conference on Programming Language
        Design and Implementation}, pages 21-34, 1988.
    \item M. Sagiv, T. Reps, and R. Wilhelm. Solving Shape-Analysis Problems
        in Languages with Destructive Updating. In \textit{ACM
        SIGPLAN-SIGACT Symposium on Principles of Programming Languages}.
        1996.
    \item M. Sagiv, T. Reps, and R. Wilhelm. Parametric Shape Analysis via
        3-Valued Logic. In \textit{ACM Symposium on Principles of
        Programming Languages}. 2002.
    \item B. Steensgaard. Points-to Analysis in Almost Linear Time. In
        \textit{Proceedings of the 23rd ACM SIGPLAN-SIGACT symposium on
        Principles of Programming Languages}, pages 32-41, 1996.
    \item T. Tok, S. Guyer, and C. Lin. Efficient Flow-Sensitive
        Interprocedural Data-Flow Analysis in the Presence of Pointers. In
        \textit{LNCS, Springer-Verlag}, pages 17-31, 2006.
    \item X. Zhang, M. Naik, and H. Yang. Finding Optimum Abstractions in
        Parametric Dataflow Analysis. In \textit{Proceedings of the 34th ACM
        SIGPLAN conference on Programming Language Design and
        Implementation}, pages 365-375, 2013.
\end{enumerate}

\section*{Notes}

:: Survey Guidelines

1) Pick an area in which you are interested. Alternatively, pick a paper
   from a recent POPL or PLDI conference. 
2) Read thoroughly 3-6 papers (or a monograph?). Read at least superficially
   3-6 other papers. 
3) Write a report on what you have learned (max 6 pages)
        - What are the basic problems
        - What are the basic approaches to solving them
        - What are the main achievements to date
        - What are open problems
Keep the scope narrow enough so you can say something interesting, and cover
2-3 lectures worth of material.

===========================================================================

Topic: shape analysis.

:: What is the goal of shape analysis?

:: Why is this important?

Specific applications of shape analysis techniques include detecting null
pointer dereferences, faulty handling of memory deallocation or destruction,
and memory leaks.

:: Is shape analysis a hard problem? Why (what are the 'basic problems')?

Undecidibility, false positives, conservativeness, time bounds (?).

Loss of precision due to abstract interpretation, simplifications which do not
reflect the properties of real-world programs (e.g ignore NULL dereferences,
treat allocations as cons cells).

:: What are the general approaches to performing shape analysis?

- L\&H: Construct an `alias graph' using abstract interpretation over a lattice
  structure, then answer structural queries by traversing the graph.
- M\&S: Construct a `shape graph' via similar means. The meet operation of the
  shape graph lattice differs from that of L\&H, and indeed the structure of the
  shape nodes is also very different. This class of graph is still constructed
  via similar means (i.e abstract interpretation, with a lattice structure to
  merge graphs).

:: What is the current state-of-the-art in shape analysis?

- Candidate: shape graphs of M\&S, see their discussion.
- Candidate: a more recent paper by M\&S which concerns parameterized 3-valued
  logic.

- Mention demand-driven techniques (Atkinson, Griswold, 1998)

:: What unsolved problems remain? How can we make progress in this field?

- Make it faster
    - Select data structures with thought for the underlying architecture
    - Create parallel analyses
- Make it more precise
- Make it more usable: how do we implement practical, fast analyses?
	o Papers often omit how to implement a mechanism for submitting queries
	  to the `shape graph' or equivalent data structure.  
	o Detail on efficient, parallel computation of these structures is also
	  lacking.
\end{document}
