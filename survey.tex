\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{indentfirst}
\usepackage[margin=1in]{geometry}
\usepackage[doublespacing]{setspace}

\title{A Survey of Shape Analysis Techniques}
\author{Vedant Kumar, \texttt{vsk@berkeley.edu}}

\begin{document}
\maketitle

\section{Introduction}

Shape analysis techniques statically determine whether the contents of a
program's memory can satisfy a set of structural invariants. The basic
problems in shape analysis are (1) to decompose a program into a set of
locations, (2) to conservatively determine what these locations point to,
and (3) to use this `points-to' information to uncover the shape of
in-memory structures. In short, the goal of shape analysis is to answer
questions about a program's memory-usage patterns without actually running
it.

The ability to answer these questions is powerful and broadly applicable.
For example, in the field of compile-time optimization the task of
automatically transforming a program into parallel fragments requires shape
analysis to flag conflicting memory operations [LH88]. The graph structures
used in several shape analysis methods (e.g [HPR89], [LH88], and [SRW96])
are amenable to solving this problem, and related dependence analysis
problems. In the field of program verification, shape analysis can be used
to check if variables satisfy sophisticated invariants such as `is-a-list?',
`is-a-tree?', and `contains-cycles?' [SRW96]. Static enforcement of such
shape invariants is useful.

This paper is organized in a top-down fashion. So far in Section 1 we have
characterized shape analysis, stated the basic problems in the field, and
motivated further study. Section 2 delves into the fundamental approaches to
solving shape analysis problems. Section 3 discusses the main achievements
in the field. Section 4 presents the challenges remaining in the field, and
Section 5 concludes.

\section{Solving shape analysis problems}

There are different approaches to solving the shape analysis problem.
Instead of describing each approach individually, we discuss their
commonalities and make generalizations whenever appropriate. In this
section, we focus on the following recurring motifs: specialized program
representation, abstract interpretation, and the construction of shape
graphs via lattice operations.

\subsection{Program representation}

The choice of program representation affects the scope and complexity of any
analysis. In the literature there are two common classes of representations.
The first is the Lisp-y \texttt{cons}-cell model espoused by [JM82], [LH88],
and [SRW96]. The other is the C-like model employed by [Steen95], [AG01],
and [KS13]. The Lisp model handles loads, stores, reference passing, and
recursive data structures.  The C model is similar, but considers pointer
dereferencing and a slew of unsafe memory operations as well. While the C
model enables analysis of a larger class of programs, some authors eschew it
in order to focus on fundamentals. Apart from the choice of program
representation, preprocessing is a common way to simplify analyses. For
example,  `kill' instructions are injected before all assignments in [SRW96]
to make the dataflow relations more compact. Such `frontend-level'
differences can be significant and drive diversity in the literature.

\subsection{Abstract interpretation}

Abstract interpretation is akin to normal program interpretation (i.e
execution), but with a specialized operational semantics. The key ideas are
(1) to replace concrete program values with abstract values, and (2) to 
simulate the resulting program under the new operational semantics. Shape
analysis algorithms utilize abstract interpretation to compute dataflow
relations between the various locations in a program, resulting in a model
of in-memory structures.

The first step in abstract interpretation is to safely replace concrete
program values with abstract ones. In [JM82] \footnote{To focus our
discussion, we treat definitions from [JM82] as representative throughout 
this subsection.}, the authors specify a concrete-value lattice $A$, an
accompanying abstract approximation lattice $A'$, and a pair of functions
which translate between the two lattices: $abs : A \rightarrow A'$ and $conc
: A' \rightarrow A$. They then state an important \textit{safe approximation
criterion}: given an $n$-ary concrete operation $\varphi : A^n \rightarrow
A$, an approximation operation $\varphi' : (A')^n \rightarrow A'$ is safe if
for all $a_1, ..., a_n$: $\{abs(\varphi(a_1, ..., a_n)) \mid \forall i.  a_i
\in conc(a'_i)\} \subseteq \varphi'(a'_1, ..., a'_n)$. The intuition behind
this requirement is that the abstract $n$-ary operation must contain all
values that result from abstracting any feasible concrete version of the
abstract operation. All operations in the abstract operational semantics
must satisfy this criterion.

The second step in abstract interpretation is to define concrete and
abstract versions of the program state.  The concrete state of a program is
an element $\sigma \in Q \times A \times L$, where $Q$ is the set of
control-flow edges and $L$ is the set of locations. The abstract state
replaces the location set with the token set $T$. In addition, each state is
equipped with a partial \textit{retrieval function} ($\tau : T \rightarrow
A' \times 2^{T \times T}$), which either maps tokens to atomic abstract
values, or to two more tokens. These $\tau$-functions allow the abstract
representation to model two program data types: atomic values and binary
lists.

The third step in abstract interpretation is to define the semantic action
of each program construct. This is a difficult but mechanical step, so we
curtail its discussion here. It suffices to say that coupled with an
equivalence relation on abstract states, these semantics allow computation
of the subset $S' \subseteq \Delta$ of reachable states in a program.  The
set $\Delta$ is ordered by subset inclusion: by the Tarski-Knaster theorem,
a least fixed-point to a function $f: \Delta \rightarrow \Delta$ exists
provided that there are no infinitely ascending chains in $\Delta$ and
that $f$ is continuous. Let $f$ be our abstract simulation function: we 
iterate it until the least fixed-point is reached, thereby completing the
abstract interpretation. This simulation can capture accurate descriptions
of recursive data structures, perform interprocedural analysis, and trade
speed for precision by adjusting its token sets. It is a flexible and
foundational concept.

\subsection{Shape graphs}

Shape graphs are an alternate form of abstract state representation
introduced in [SRW96]. We focus on shape graphs because they have been used
to drive key advances in the field. The shape graph is conceptually similar to
the alias graph in [LH88] and (to some extent) the retrieval functions of
[JM82], so our discussion will not be too idiosyncratic. We define shape
graphs \footnote{All results in this subsection are drawn directly from
[SRW96], unless explicitly mentioned otherwise.}, examine how they are
constructed, discuss some interesting properties, and compare them to other
abstract state representations.

The shape graph is a finite digraph consisting of \textit{shape nodes},
\textit{variable edges}, and \textit{selector edges}. It is formally defined
as a tuple $\langle E_s, E_v \rangle$ in [SRW96]. Variable edges of the form
$[x, n]$ reside in $E_v$, where $x$ is a pointer variable and $n$ is a shape
node (i.e a graph vertex). Selector edges of the form $[s, sel, t]$ reside
in $E_s$, where $sel$ is a selector and $s$ and $t$ are shape nodes. The
sets $E_v$ and $E_s$ fully describe the shape graph. The class of
deterministic shape graphs is $\mathcal{DSG}$: it contains graphs which may
only represent the ephemeral effects of one execution sequence.  Formally,
graphs in $\mathcal{DSG}$ satisfy $\forall x.  |E_v(x)| \leq 1$ and $\forall
x.  \forall sel. |E_s(x, sel)| \leq 1$. We require a separate class of
static shape graphs to conservatively represent subsets of $\mathcal{DSG}$
for our abstract semantics. This new class is the lattice $\mathcal{SSG}$,
ordered by component-wise subset inclusion. 

Our goal of computing useful static program representations is within reach.
First, we define a concrete semantics as a
$\mathcal{DSG}$-transformer: $[\![st]\!]_{\mathcal{DSG}}: \mathcal{DSG}
\rightarrow \mathcal{DSG}$.  Second, we represent the control flow nodes of
the program as a set $V$.  Third, we define a \textit{collecting semantics},
$c : V \rightarrow 2^{\mathcal{DSG}}$, which generates all feasible
deterministic shape graphs for a given program point. Explicitly, $c(v) =
\{[\![st(v_k)]\!]_{\mathcal{DSG}}(...,
[\![st(v_1)]\!]_{\mathcal{DSG}}(\langle \phi, \phi \rangle)) \mid v_1, ...,
v_k \in pathsTo(v)\}$, where $pathsTo : V \rightarrow 2^V$ yields all paths
through the CFG which may transition to $v$. Next, we define an abstraction
function, $\alpha : 2^{\mathcal{DSG}} \rightarrow \mathcal{SSG}$. Finally,
we define the abstract semantics as a $\mathcal{SSG}$-transformer:
$[\![st]\!]_{\mathcal{SSG}}: \mathcal{SSG} \rightarrow \mathcal{SSG}$.
Putting all the pieces together, the shape analysis algorithm can be thought
of as the function composition $\alpha \circ c : V \rightarrow
2^{\mathcal{DSG}} \rightarrow \mathcal{SSG}$ followed by a fixed-point
iteration. A discussion of the exact concrete and abstract semantics is
omitted because it would require pages of dense equations and exposition.
For reference, figures 2 and 6 in [SRW96] fully specify 
$[\![st]\!]_{\mathcal{DSG}, \mathcal{SSG}}$.

Shape graphs have interesting properties, such as a fluid naming scheme and
strong nullification. The naming scheme is simple: the shape nodes at a
given CFG node $v$ are named by the set of $v$-local variables which all
point to the same run-time location.  Variables can be added, removed, and
moved between shape nodes to model $\mathcal{DSG}$-transformations as
precisely as possible. Shape nodes are not forced to irreversibly partion
memory with a fixed variable labelling: they may be \textit{materialized}
(split into more granular nodes) as well as \textit{un-materialized}
(coalesced into a summary node) as necessary.  The variable-set naming
scheme also enables our second property, \textit{strong nullification}, the
condition that all variable edges emanating from a shape node are removed on
a $nil$-assignment. This can be expressed as $[\![x :=
\textbf{nil}]\!]_{\mathcal{SSG}}(\langle E_v, E_s \rangle) \approx \langle
E_v - [x, *], E_s \rangle$ \footnote{Some details have been omitted here for
simplicity.} where all shape nodes $n_X$ s.t $x \in X$ are renamed to $n_{X
- \{x\}}$.  Strong nullification is cited as a crucial factor in enabling
the model-checking of the `is-list?' invariant.

In comparison to the method described in [JM82], the shape graph formalism
permits more flexibility in terms of supported structural invariants. The
language of retrieval functions would be difficult to extend for these
purposes. In comparison to the alias graphs presented in [LH88], shape
graphs have a simplfied naming scheme which dispenses with access path
concatenation and aggregate labels (i.e node names containing regular
expressions). Ironically, the method presented in [LH88] solves the
alias-analysis problem in order to detect structure access conflicts, while
alias-analysis is described as a feasible extension of the method in
[SRW96].

\section{Main achivements}

The results in [SRW02] stand out as major achievements in the field. The
authors were able to design a logical framework which characterizes a wide
range of structural invariants. These invariants are highly expressive: they
may encode type information, structure connectivity properties, and ordering
properties. In addition, the authors designed a domain-specific language in
which these invariants may be expressed. An analyzer-generator was
implemented to transform these specially-crafted invariants into a
conservative shape analyzer. The shape analysis algorithm itself builds upon
previous work.

Shape analysis techniques may be applied interprocedurally, due to
consistent consideration throughout the literature (e.g in [JM82], [LH88],
[SRW*], etc). Work has also been done to reduce the space complexity of
shape analysis: merged shape nodes are discussed in [SRW96], while summary
nodes and compact hammock graphs are discussed in [LH88]. In [AG98],
implmentation techniques are established to make dataflow analysis more
efficient (notably demand-driven analysis). The popular work from [Steen95],
Steensgaard's almost linear-time points-to algorithm, has been widely
implemented (e.g in the LLVM compiler framework).

\section{Current limitations}

The perfect shape analysis algorithm produces a deterministic shape graph
that accurately describes the contents of memory at any selected point in
time. Three program features make such an algorithm infeasible:
non-determinism, destructive updating, and recursion. 

\begin{enumerate}[1.]
    \item handling context sensitivity / interprocedural => hard
    \item size of structures (s/l limits, k-limiting, merging shape nodes)
    \item complexity of context sensitive analysis
    \item NP-hardness of approximation (see Larus/Hilfinger).
    \item Concerns about tractability.
    \item Usually deals with heap-allocated storage, ignores the stack, and
        other types of (potentially volatile) memory.
    \item NULL pointer dereferences.
    \item Dealing with unsafe deallocations (\texttt{free()}).
    \item Difficulty of implementation.
    \item Constructing a conservative approximation quickly.
\end{enumerate}

\section{Conclusion}

Fastest?
Most space-efficient?
Best complexity?
Most precise?

Future work;
\begin{enumerate}[1.]
    \item Designing parallel shape analysis algorithms.
    \item Determine the best way to trade precision for speed.
    \item Overcoming as many of the limitations mentioned above as possible.
\end{enumerate}

Shape analyses determine the structure of dynamically-updated storage
statically.

Most immediately, it lets us answer questions about how a program organizes
memory without running it. For example, we may specify a set of invariants
for objects in memory and determine whether or not these invariants are ever
violated (i.e we may model-check data structures). Shape analysis enables
invariants which constrain memory accesses, allowing us to answer relevant
questions about memory sharing and reachability in programs.

\section{Bibliography}

\begin{enumerate}[1.]
    \item U. A{\ss}mann and M. Weinhardt. Interprocedural Heap Analysis for
        Parallelizing Imperative Programs. In \textit{IEEE, Programming
        Models for Massively Parallel Computers}, pages 74-82, 1993.
    \item D. Atkinson and W. Griswold. Effective Whole-Program Analysis in
        the Presence of Pointers. In \textit{SIGSOFT '98/FSE-6 Proceedings
        of the 6th ACM SIGSOFT'}, pages 46-55, 1998.
    \item D. Atkinson and W. Griswold. Implementation Techniques for
        Efficient Data-Flow Analysis of Large Programs. In
        \textit{Proceedings of the International Conference on Software
        Maintenance}, pages 52-61, 2001.
    \item S. Horwitz, P. Pfeiffer, and T. Reps. Dependence Analysis for
        Pointer Variables. In \textit{Proceedings of the ACM SIGPLAN 
        Conference on Programming Language Design and Implementation}, pages
        28-40, 1989.
    \item N. Jones and S. Muchnick. A Flexible Approach to Interprocedural
        Data Flow Analysis and Programs with Recursive Data Structures. In
        \textit{ACM Symposium on Principles of Programming Languages}, pages
        66-74, 1982.
    \item G. Kastrinis and Y. Smaragdakis. Hybrid Context-Sensitivity for
        Points-To Analysis. In \textit{Proceedings of the 34th ACM SIGPLAN
        conference on Programming Language Design and Implementation}, pages
        423-434, 2013.
    \item J. Larus and P. Hilfinger. Detecting Conflicts Between Structure
        Accesses. In \textit{SIGPLAN Conference on Programming Language
        Design and Implementation}, pages 21-34, 1988.
    \item M. Sagiv, T. Reps, and R. Wilhelm. Solving Shape-Analysis Problems
        in Languages with Destructive Updating. In \textit{ACM
        SIGPLAN-SIGACT Symposium on Principles of Programming Languages}.
        1996.
    \item M. Sagiv, T. Reps, and R. Wilhelm. Parametric Shape Analysis via
        3-Valued Logic. In \textit{ACM Symposium on Principles of
        Programming Languages}. 2002.
    \item B. Steensgaard. Points-to Analysis in Almost Linear Time. In
        \textit{Proceedings of the 23rd ACM SIGPLAN-SIGACT symposium on
        Principles of Programming Languages}, pages 32-41, 1996.
    \item T. Tok, S. Guyer, and C. Lin. Efficient Flow-Sensitive
        Interprocedural Data-Flow Analysis in the Presence of Pointers. In
        \textit{LNCS, Springer-Verlag}, pages 17-31, 2006.
    \item X. Zhang, M. Naik, and H. Yang. Finding Optimum Abstractions in
        Parametric Dataflow Analysis. In \textit{Proceedings of the 34th ACM
        SIGPLAN conference on Programming Language Design and
        Implementation}, pages 365-375, 2013.
\end{enumerate}

\section*{Notes}

:: Survey Guidelines

3) Write a report on what you have learned (max 6 pages)
        - What are the basic problems
        - What are the basic approaches to solving them
        - What are the main achievements to date
        - What are open problems
Keep the scope narrow enough so you can say something interesting, and cover
2-3 lectures worth of material.

===========================================================================

Topic: shape analysis.

:: Is shape analysis a hard problem? Why (what are the 'basic problems')?

Undecidibility, false positives, conservativeness, time bounds (?).

Loss of precision due to abstract interpretation, simplifications which do not
reflect the properties of real-world programs (e.g ignore NULL dereferences,
treat allocations as cons cells).

:: What is the current state-of-the-art in shape analysis?

- Candidate: a more recent paper by M\&S which concerns parameterized 3-valued
  logic.

:: What unsolved problems remain? How can we make progress in this field?

- Make it faster
    - Select data structures with thought for the underlying architecture
    - Mention demand-driven techniques (Atkinson, Griswold, 1998)
    - Create parallel analyses
- Make it more precise

\end{document}
