\documentclass{article}

\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{indentfirst}
\usepackage[margin=1in]{geometry}
\usepackage[doublespacing]{setspace}

\title{A Survey of Shape Analysis Techniques}
\author{Vedant Kumar, \texttt{vsk@berkeley.edu}}

\begin{document}
\maketitle

\section{Introduction}

Shape analysis techniques statically determine whether the contents of a
program's memory can satisfy a set of structural invariants. The basic
problems in shape analysis are (1) to decompose a program into a set of
locations, (2) to conservatively determine what these locations point to,
and (3) to use this `points-to' information to uncover the shape of
in-memory structures. In short, the goal of shape analysis is to answer
questions about a program's memory-usage patterns without actually running
it.

The ability to answer these questions is powerful and broadly applicable.
For example, in the field of compile-time optimization the task of
automatically transforming a program into parallel fragments requires shape
analysis to flag conflicting memory operations [LH88]. The graph structures
used in several shape analysis methods (e.g [HPR89], [LH88], and [SRW96])
are amenable to solving not just this problem, but other related
dependency-tracking problems which arise in optimization. In the field of
program verification, shape analysis can be used to check if variables
satisfy sophisticated invariants such as `is-a-list?', `is-a-tree?', and
`contains-cycles?' [SRW96]. Enforcing these shape invariants statically is
useful. 

This paper is organized in a top-down fashion. So far in Section 1 we have
characterized shape analysis, stated the basic problems in the field, and
motivated further study. Section 2 delves into the fundamental approaches to
solving shape analysis problems. Section 3 discusses the achievements of the
field and the state of the art. Section 4 presents the challenges remaining
in the field. Section 5 concludes.

\section{Solving shape analysis problems}

The wealth of research in this field has lead to multiple different
approaches to solving the shape analysis problem. Instead of describing each
approach individually, we discuss their commonalities and make
generalizations whenever appropriate. In this section, we focus on the
following recurring motifs: specialized program representation, abstract
interpretation, and the construction of a shape graph via lattice
operations.

\subsection{Program representation}

The choice of program representation has affects the scope and complexity of
analysis algorithms. In the literature, there are two common classes of
representations. The first is the Lisp-y \texttt{cons}-cell model espoused
by [JM82], [LH88], and [SRW96]. The other is the C-like model employed by
[Steen95], [AG01], and [KS13]. The Lisp model focuses on the challenges
posed by loads, stores, reference passing, and recursive data structures.
The C model complicates the Lisp model with pointer dereferencing and a slew
of unsafe memory operations. While this model enables analysis of a larger
class of programs, some authors eschew it in order to focus on fundamentals.
Other program preprocessing steps exist, such as the injection of `kill'
instructions before all assignments (as seen in [SRW96]). This simple trick
leads to simplified data-flow equations as compared to those in [LH88]. Such
`frontend-level' differences are useful and drive diversity in the
literature, but are ultimately engineering concerns.

\subsection{Abstract interpretation}

Abstract interpretation is akin to normal program interpretation (i.e
execution), but with a specialized operational semantics. The key ideas are
(1) to replace concrete program values with abstract values from a lattice,
and (2) to then simulate the resulting program using the new operational
semantics until a fixed-point criterion is met. Shape analysis algorithms
utilize abstract interpretation to compute dataflow relations between the
various locations in a program, resulting in a model of the in-memory
structures of a program.

The first step in abstract interpretation is to safely replace concrete
program values with abstract ones. In [JM82] \footnote{We focus on
definitions from [JM82] to guide our discussion throughout this
subsection.}, the authors specify a concrete-value lattice $A$, an
accompanying abstract approximation lattice $A'$, and a pair of functions
which translate values between the two lattices: $abs : A \rightarrow A'$
and $conc : A' \rightarrow A$. Then they state an important \textit{safe
approximation criterion}: given an $n$-ary concrete operation $\varphi : A^n
\rightarrow A$, an approximation operation $\varphi' : (A')^n \rightarrow
A'$ is safe if for all $a_1, ..., a_n$: $\{abs(\varphi(a_1, ..., a_n)) \mid
\forall i. a_i \in conc(a'_i)\} \subseteq \varphi'(a'_1, ..., a'_n)$. The
intuition behind this requirement is that the abstract $n$-ary operation
must contain all values that result from abstracting any feasible concrete
version of the abstract operation. This is a general criterion which sound
abstraction function must comply with (e.g the $combine_{ij}$ rule for
merging abstract environments in \texttt{escape} and \texttt{return}
operations rely on this property).

The second step in abstract interpretation is to define concrete and
abstract versions of the program state \footnote{Our semantics discard
concrete values so handling abstract program states is necessary.}. The
concrete state of a program is an element $\sigma \in Q \times A \times L$,
where $Q$ is the set of control-flow edges, $A$ is the set of concrete
values, and $L$ is the set of locations. The abstract state replaces the
location set with the token set $T$. A tunable number of tokens are used to
represent locations in the program: increasing the number of tokens results
in a more precise, albeit more expensive analysis. In addition to using
tokens, abstract states are equipped with partial \textit{retrieval
functions} ($\tau : T \rightarrow A' \times 2^{T \times T}$), which map
tokens to either atomic abstract values, or to two more tokens. These
$\tau$-functions allow the abstract representation to track the two program
data types: atomic values and binary lists.

The third step in abstract interpretation is to define the semantic action
of each program construct. This is a long and mechanical step, so we cover
it in more detail in the next subsection on shape graphs and curtail the
discussion here. It suffices to say that coupled with an equivalence
relation between abstract states, these semantics allow computation of the
set of reachable abstract states in a program. The abstract interpretation
is actually effected by a simulation function $f : \Delta \rightarrow
\Delta$, where $\Delta$ is the set of abstract states. The interpretation
terminates when a least fixed point is found. The set $\Delta$ is ordered by
subset inclusion: existence of the least fixed point follows from the
Tarski-Knaster theorem provided $f$ is monotone and that there are no
infinitely ascending chains in $\Delta$.

\subsection{Shape graph lattices}

The abstract interpretation showcased in [JM82] is very flexible and
foundational. It can capture accurate descriptions of recursive data
structures, perform interprocedural analysis, and trade speed for precision
by adjusting its token sets.


\textit{Shape graphs} are the fundamental data structure used in shape
analysis 
    \footnote{Shape graphs are formally defined in [SRW96], and we treat
        this work as representative. Other authors use conceptually similar
        objects (e.g alias graphs in [LH88], retrieval functions in [JM82],
        etc), so this is not an egregious claim.}.
The vertices of the graph are program locations or `shape nodes'.  There
directed edges in the graph encode reachability. Usually, shape graphs are
equipped with multiple types of edges to handle language features such as
field and array selectors.

    Jones and Muchnick generate tokens at strategic points in a program to
    represent locations. They are able to vary the precision of their
    analysis by altering the number of tokens generated. 

    Larus and Hilfginer construct an `alias graph'

The shape graph is the central object in 

\begin{enumerate}[1.]
    \item Shape graphs.
    \item Abstract interpretation.
    \item Concrete vs. collecting vs. abstract semantics.
    \item Dataflow equations.
    \item Lattice structure of shape graphs.
    \item Modelling interprocedural control flow.
    \item Handling context-sensitivity.
    \item Handling destructive pointer updates.
\end{enumerate}

\section{The state of the art}

Fastest?
Most space-efficient?
Best complexity?
Most precise?

Mention invariants Sagiv/Reps are able to capture.
Discussion of Steensgaard here.
Discussion of context sensitive/insensitive.

\section{Current limitations}

The perfect shape analysis algorithm produces a deterministic shape graph
that accurately describes the contents of memory at any selected point in
time. Three program features make such an algorithm infeasible:
non-determinism, destructive updating, and recursion. 

\begin{enumerate}[1.]
    \item NP-hardness of approximation (see Larus/Hilfinger).
    \item Concerns about tractability.
    \item Disambiguating between heap-allocated storage, the stack, and
        other types of (potentially volatile) memory.
    \item NULL pointer dereferences.
    \item Dealing with unsafe deallocations (\texttt{free()}).
    \item Difficulty of implementation.
    \item Constructing a conservative approximation quickly.
\end{enumerate}

\section{Conclusion}

Future work;
\begin{enumerate}[1.]
    \item Designing parallel shape analysis algorithms.
    \item Determine the best way to trade precision for speed.
    \item Overcoming as many of the limitations mentioned above as possible.
\end{enumerate}

Shape analyses determine the structure of dynamically-updated storage
statically.

Most immediately, it lets us answer questions about how a program organizes
memory without running it. For example, we may specify a set of invariants
for objects in memory and determine whether or not these invariants are ever
violated (i.e we may model-check data structures). Shape analysis enables
invariants which constrain memory accesses, allowing us to answer relevant
questions about memory sharing and reachability in programs.

\section{Bibliography}

\begin{enumerate}[1.]
    \item U. A{\ss}mann and M. Weinhardt. Interprocedural Heap Analysis for
        Parallelizing Imperative Programs. In \textit{IEEE, Programming
        Models for Massively Parallel Computers}, pages 74-82, 1993.
    \item D. Atkinson and W. Griswold. Effective Whole-Program Analysis in
        the Presence of Pointers. In \textit{SIGSOFT '98/FSE-6 Proceedings
        of the 6th ACM SIGSOFT'}, pages 46-55, 1998.
    \item D. Atkinson and W. Griswold. Implementation Techniques for
        Efficient Data-Flow Analysis of Large Programs. In
        \textit{Proceedings of the International Conference on Software
        Maintenance}, pages 52-61, 2001.
    \item S. Horwitz, P. Pfeiffer, and T. Reps. Dependence Analysis for
        Pointer Variables. In \textit{Proceedings of the ACM SIGPLAN 
        Conference on Programming Language Design and Implementation}, pages
        28-40, 1989.
    \item N. Jones and S. Muchnick. A Flexible Approach to Interprocedural
        Data Flow Analysis and Programs with Recursive Data Structures. In
        \textit{ACM Symposium on Principles of Programming Languages}, pages
        66-74, 1982.
    \item G. Kastrinis and Y. Smaragdakis. Hybrid Context-Sensitivity for
        Points-To Analysis. In \textit{Proceedings of the 34th ACM SIGPLAN
        conference on Programming Language Design and Implementation}, pages
        423-434, 2013.
    \item J. Larus and P. Hilfinger. Detecting Conflicts Between Structure
        Accesses. In \textit{SIGPLAN Conference on Programming Language
        Design and Implementation}, pages 21-34, 1988.
    \item M. Sagiv, T. Reps, and R. Wilhelm. Solving Shape-Analysis Problems
        in Languages with Destructive Updating. In \textit{ACM
        SIGPLAN-SIGACT Symposium on Principles of Programming Languages}.
        1996.
    \item M. Sagiv, T. Reps, and R. Wilhelm. Parametric Shape Analysis via
        3-Valued Logic. In \textit{ACM Symposium on Principles of
        Programming Languages}. 2002.
    \item B. Steensgaard. Points-to Analysis in Almost Linear Time. In
        \textit{Proceedings of the 23rd ACM SIGPLAN-SIGACT symposium on
        Principles of Programming Languages}, pages 32-41, 1996.
    \item T. Tok, S. Guyer, and C. Lin. Efficient Flow-Sensitive
        Interprocedural Data-Flow Analysis in the Presence of Pointers. In
        \textit{LNCS, Springer-Verlag}, pages 17-31, 2006.
    \item X. Zhang, M. Naik, and H. Yang. Finding Optimum Abstractions in
        Parametric Dataflow Analysis. In \textit{Proceedings of the 34th ACM
        SIGPLAN conference on Programming Language Design and
        Implementation}, pages 365-375, 2013.
\end{enumerate}

\section*{Notes}

:: Survey Guidelines

1) Pick an area in which you are interested. Alternatively, pick a paper
   from a recent POPL or PLDI conference. 
2) Read thoroughly 3-6 papers (or a monograph?). Read at least superficially
   3-6 other papers. 
3) Write a report on what you have learned (max 6 pages)
        - What are the basic problems
        - What are the basic approaches to solving them
        - What are the main achievements to date
        - What are open problems
Keep the scope narrow enough so you can say something interesting, and cover
2-3 lectures worth of material.

===========================================================================

Topic: shape analysis.

:: What is the goal of shape analysis?

:: Why is this important?

Specific applications of shape analysis techniques include detecting null
pointer dereferences, faulty handling of memory deallocation or destruction,
and memory leaks.

:: Is shape analysis a hard problem? Why (what are the 'basic problems')?

Undecidibility, false positives, conservativeness, time bounds (?).

Loss of precision due to abstract interpretation, simplifications which do not
reflect the properties of real-world programs (e.g ignore NULL dereferences,
treat allocations as cons cells).

:: What are the general approaches to performing shape analysis?

- L\&H: Construct an `alias graph' using abstract interpretation over a lattice
  structure, then answer structural queries by traversing the graph.
- M\&S: Construct a `shape graph' via similar means. The meet operation of the
  shape graph lattice differs from that of L\&H, and indeed the structure of the
  shape nodes is also very different. This class of graph is still constructed
  via similar means (i.e abstract interpretation, with a lattice structure to
  merge graphs).

:: What is the current state-of-the-art in shape analysis?

- Candidate: shape graphs of M\&S, see their discussion.
- Candidate: a more recent paper by M\&S which concerns parameterized 3-valued
  logic.

- Mention demand-driven techniques (Atkinson, Griswold, 1998)

:: What unsolved problems remain? How can we make progress in this field?

- Make it faster
    - Select data structures with thought for the underlying architecture
    - Create parallel analyses
- Make it more precise
- Make it more usable: how do we implement practical, fast analyses?
	o Papers often omit how to implement a mechanism for submitting queries
	  to the `shape graph' or equivalent data structure.  
	o Detail on efficient, parallel computation of these structures is also
	  lacking.
\end{document}
