\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{indentfirst}
\usepackage[margin=1in]{geometry}
\usepackage[doublespacing]{setspace}

\title{A Survey of Shape Analysis Techniques}
\author{Vedant Kumar, \texttt{vsk@berkeley.edu}}

\begin{document}
\maketitle

\section{Introduction}

Shape analysis techniques statically determine whether the contents of a
program's memory can satisfy a set of structural invariants. The basic
problems in shape analysis are (1) to decompose a program into a set of
locations, (2) to conservatively determine what these locations point to,
and (3) to use this `points-to' information to uncover the shape of
in-memory structures. In short, the goal of shape analysis is to answer
questions about a program's memory-usage patterns without actually running
it.

The ability to answer these questions is powerful and broadly applicable.
For example, in the field of compile-time optimization the task of
automatically transforming a program into parallel fragments requires shape
analysis to flag conflicting memory operations [LH88]. The graph structures
used in several shape analysis methods (e.g [HPR89], [LH88], and [SRW96])
are amenable to solving this problem, and related dependence analysis
problems. In the field of program verification, shape analysis can be used
to check if variables satisfy sophisticated invariants such as `is-a-list?',
`is-a-tree?', and `contains-cycles?' [SRW96]. Static enforcement of such
shape invariants is useful.

This paper is organized in a top-down fashion. So far in Section 1 we have
characterized shape analysis, stated the basic problems in the field, and
motivated further study. Section 2 delves into the fundamental approaches to
solving shape analysis problems. Section 3 discusses the main achievements
in the field. Section 4 presents the challenges remaining in the field, and
Section 5 concludes.

\section{Solving shape analysis problems}

There are different approaches to solving the shape analysis problem.
Instead of describing each approach individually, we discuss their
commonalities and make generalizations whenever appropriate. In this
section, we focus on the following recurring motifs: specialized program
representation, abstract interpretation, and the construction of shape
graphs via lattice operations.

\subsection{Program representation}

The choice of program representation affects the scope and complexity of any
analysis. In the literature there are two common classes of representations.
The first is the Lisp-y \texttt{cons}-cell model espoused by [JM82], [LH88],
and [SRW96]. The other is the C-like model employed by [Steen95], [AG01],
and [KS13]. The Lisp model handles loads, stores, reference passing, and
recursive data structures.  The C model is similar, but considers pointer
dereferencing and a slew of unsafe memory operations as well. While the C
model enables analysis of a larger class of programs, some authors eschew it
in order to focus on fundamentals. Apart from the choice of program
representation, preprocessing is a common way to simplify analyses. For
example,  `kill' instructions are injected before all assignments in [SRW96]
to make the dataflow relations more compact. Such `frontend-level'
differences can be significant and drive diversity in the literature.

\subsection{Abstract interpretation}

Abstract interpretation is akin to normal program interpretation (i.e
execution), but with a specialized operational semantics. The key ideas are
(1) to replace concrete program values with abstract values, and (2) to 
simulate the resulting program under the new operational semantics. Shape
analysis algorithms utilize abstract interpretation to compute dataflow
relations between the various locations in a program, resulting in a model
of in-memory structures.

The first step in abstract interpretation is to safely replace concrete
program values with abstract ones. In [JM82] \footnote{To focus our
discussion, we treat definitions from [JM82] as representative throughout 
this subsection.}, the authors specify a concrete-value lattice $A$, an
accompanying abstract approximation lattice $A'$, and a pair of functions
which translate between the two lattices: $abs : A \rightarrow A'$ and $conc
: A' \rightarrow A$. They then state an important \textit{safe approximation
criterion}: given an $n$-ary concrete operation $\varphi : A^n \rightarrow
A$, an approximation operation $\varphi' : (A')^n \rightarrow A'$ is safe if
for all $a_1, ..., a_n$: $\{abs(\varphi(a_1, ..., a_n)) \mid \forall i.  a_i
\in conc(a'_i)\} \subseteq \varphi'(a'_1, ..., a'_n)$. The intuition behind
this requirement is that the abstract $n$-ary operation must contain all
values that result from abstracting any feasible concrete version of the
abstract operation. All operations in the abstract operational semantics
must satisfy this criterion.

The second step in abstract interpretation is to define concrete and
abstract versions of the program state.  The concrete state of a program is
an element $\sigma \in Q \times A \times L$, where $Q$ is the set of
control-flow edges and $L$ is the set of locations. The abstract state
replaces the location set with the token set $T$. In addition, each state is
equipped with a partial \textit{retrieval function} ($\tau : T \rightarrow
A' \times 2^{T \times T}$), which either maps tokens to atomic abstract
values, or to two more tokens. These $\tau$-functions allow the abstract
representation to model two program data types: atomic values and binary
lists.

The third step in abstract interpretation is to define the semantic action
of each program construct. This is a difficult but mechanical step, so we
curtail its discussion here. It suffices to say that coupled with an
equivalence relation on abstract states, these semantics allow computation
of the subset $S' \subseteq \Delta$ of reachable states in a program.  The
set $\Delta$ is ordered by subset inclusion: by the Tarski-Knaster theorem,
a least fixed-point to a function $f: \Delta \rightarrow \Delta$ exists
provided that there are no infinitely ascending chains in $\Delta$ and
that $f$ is continuous. Let $f$ be our abstract simulation function: we 
iterate it until the least fixed-point is reached, thereby completing the
abstract interpretation. This simulation can capture accurate descriptions
of recursive data structures, perform interprocedural analysis, and trade
speed for precision by adjusting its token sets. It is a flexible and
foundational concept.

\subsection{Shape graphs}

Shape graphs are an alternate form of abstract state representation
introduced in [SRW96]. We focus on shape graphs because they have been used
to drive key advances in the field. The shape graph is conceptually similar to
the alias graph in [LH88] and (to some extent) the retrieval functions of
[JM82], so our discussion will not be too idiosyncratic. We define shape
graphs \footnote{All results in this subsection are drawn directly from
[SRW96], unless explicitly mentioned otherwise.}, examine how they are
constructed, discuss some interesting properties, and compare them to other
abstract state representations.

The shape graph is a finite digraph consisting of \textit{shape nodes},
\textit{variable edges}, and \textit{selector edges}. It is formally defined
as a tuple $\langle E_s, E_v \rangle$ in [SRW96]. Variable edges of the form
$[x, n]$ reside in $E_v$, where $x$ is a pointer variable and $n$ is a shape
node (i.e a graph vertex). Selector edges of the form $[s, sel, t]$ reside
in $E_s$, where $sel$ is a selector and $s$ and $t$ are shape nodes. The
sets $E_v$ and $E_s$ fully describe the shape graph. The class of
deterministic shape graphs is $\mathcal{DSG}$: it contains graphs which may
only represent the ephemeral effects of one execution sequence.  Formally,
graphs in $\mathcal{DSG}$ satisfy $\forall x.  |E_v(x)| \leq 1$ and $\forall
x.  \forall sel. |E_s(x, sel)| \leq 1$. We require a separate class of
static shape graphs to conservatively represent subsets of $\mathcal{DSG}$
for our abstract semantics. This new class is the lattice $\mathcal{SSG}$,
ordered by component-wise subset inclusion. 

Our goal of computing useful static program representations is within reach.
First, we define a concrete semantics as a
$\mathcal{DSG}$-transformer: $[\![st]\!]_{\mathcal{DSG}}: \mathcal{DSG}
\rightarrow \mathcal{DSG}$.  Second, we represent the control flow nodes of
the program as a set $V$.  Third, we define a \textit{collecting semantics},
$c : V \rightarrow 2^{\mathcal{DSG}}$, which generates all feasible
deterministic shape graphs for a given program point. Explicitly, $c(v) =
\{[\![st(v_k)]\!]_{\mathcal{DSG}}(...,
[\![st(v_1)]\!]_{\mathcal{DSG}}(\langle \phi, \phi \rangle)) \mid v_1, ...,
v_k \in pathsTo(v)\}$, where $pathsTo : V \rightarrow 2^V$ yields all paths
through the CFG which may transition to $v$. Next, we define an abstraction
function, $\alpha : 2^{\mathcal{DSG}} \rightarrow \mathcal{SSG}$. Finally,
we define the abstract semantics as a $\mathcal{SSG}$-transformer:
$[\![st]\!]_{\mathcal{SSG}}: \mathcal{SSG} \rightarrow \mathcal{SSG}$.
Putting all the pieces together, the shape analysis algorithm can be thought
of as the function composition $\alpha \circ c : V \rightarrow
2^{\mathcal{DSG}} \rightarrow \mathcal{SSG}$ followed by a fixed-point
iteration. A discussion of the exact concrete and abstract semantics is
omitted because it would require pages of dense equations and exposition.
For reference, figures 2 and 6 in [SRW96] fully specify 
$[\![st]\!]_{\mathcal{DSG}, \mathcal{SSG}}$.

Shape graphs have numerous interesting properties. For one, the shape nodes
at a given CFG node $v$ are named by the set of $v$-local variables which
all point to the same run-time location. This naming scheme is fluid.
Variables can be added, removed, and moved between shape nodes to model
$\mathcal{DSG}$-transformations as precisely as possible. Shape nodes are
not forced to irreversibly partion memory with a fixed variable labelling:
they may be \textit{materialized} (split into more granular nodes) as well
as \textit{un-materialized} (coalesced into a summary node) as necessary.
The variable-set naming scheme enables \textit{strong nullification}, the
property that all variable edges emanating from a shape node are removed on
a $nil$-assignment. The naming scheme allows strong nullification to rename
and split/merge all shape nodes affected by the assignment. This property is
expressed as $[\![x := \textbf{nil}]\!]_{\mathcal{SSG}}(\langle E_v, E_s
\rangle) \approx \langle E_v - [x, *], E_s \rangle$ where all shape nodes
$n_X$ s.t $x \in X$ are renamed to $X - \{x\}$ (some details have been
omitted here for simplicity).  Strong nullification is cited as a crucial
factor in model-checking the `is-list?' invariant.  

Comparison
    not limited to atomis and binary lists 
    better naming scheme / LH88

\begin{enumerate}[1.]
    \item Handling interprocedural flows, context-sensitivity?
    \item Handling destructive pointer updates?
\end{enumerate}

\section{Main achivements}

Fastest?
Most space-efficient?
Best complexity?
Most precise?

Mention invariants Sagiv/Reps are able to capture.
Discussion of Steensgaard here.
Discussion of context sensitive/insensitive.

\section{Current limitations}

The perfect shape analysis algorithm produces a deterministic shape graph
that accurately describes the contents of memory at any selected point in
time. Three program features make such an algorithm infeasible:
non-determinism, destructive updating, and recursion. 

\begin{enumerate}[1.]
    \item complexity of context sensitive analysis
    \item NP-hardness of approximation (see Larus/Hilfinger).
    \item Concerns about tractability.
    \item Usually deals with heap-allocated storage, ignores the stack, and
        other types of (potentially volatile) memory.
    \item NULL pointer dereferences.
    \item Dealing with unsafe deallocations (\texttt{free()}).
    \item Difficulty of implementation.
    \item Constructing a conservative approximation quickly.
\end{enumerate}

\section{Conclusion}

Future work;
\begin{enumerate}[1.]
    \item Designing parallel shape analysis algorithms.
    \item Determine the best way to trade precision for speed.
    \item Overcoming as many of the limitations mentioned above as possible.
\end{enumerate}

Shape analyses determine the structure of dynamically-updated storage
statically.

Most immediately, it lets us answer questions about how a program organizes
memory without running it. For example, we may specify a set of invariants
for objects in memory and determine whether or not these invariants are ever
violated (i.e we may model-check data structures). Shape analysis enables
invariants which constrain memory accesses, allowing us to answer relevant
questions about memory sharing and reachability in programs.

\section{Bibliography}

\begin{enumerate}[1.]
    \item U. A{\ss}mann and M. Weinhardt. Interprocedural Heap Analysis for
        Parallelizing Imperative Programs. In \textit{IEEE, Programming
        Models for Massively Parallel Computers}, pages 74-82, 1993.
    \item D. Atkinson and W. Griswold. Effective Whole-Program Analysis in
        the Presence of Pointers. In \textit{SIGSOFT '98/FSE-6 Proceedings
        of the 6th ACM SIGSOFT'}, pages 46-55, 1998.
    \item D. Atkinson and W. Griswold. Implementation Techniques for
        Efficient Data-Flow Analysis of Large Programs. In
        \textit{Proceedings of the International Conference on Software
        Maintenance}, pages 52-61, 2001.
    \item S. Horwitz, P. Pfeiffer, and T. Reps. Dependence Analysis for
        Pointer Variables. In \textit{Proceedings of the ACM SIGPLAN 
        Conference on Programming Language Design and Implementation}, pages
        28-40, 1989.
    \item N. Jones and S. Muchnick. A Flexible Approach to Interprocedural
        Data Flow Analysis and Programs with Recursive Data Structures. In
        \textit{ACM Symposium on Principles of Programming Languages}, pages
        66-74, 1982.
    \item G. Kastrinis and Y. Smaragdakis. Hybrid Context-Sensitivity for
        Points-To Analysis. In \textit{Proceedings of the 34th ACM SIGPLAN
        conference on Programming Language Design and Implementation}, pages
        423-434, 2013.
    \item J. Larus and P. Hilfinger. Detecting Conflicts Between Structure
        Accesses. In \textit{SIGPLAN Conference on Programming Language
        Design and Implementation}, pages 21-34, 1988.
    \item M. Sagiv, T. Reps, and R. Wilhelm. Solving Shape-Analysis Problems
        in Languages with Destructive Updating. In \textit{ACM
        SIGPLAN-SIGACT Symposium on Principles of Programming Languages}.
        1996.
    \item M. Sagiv, T. Reps, and R. Wilhelm. Parametric Shape Analysis via
        3-Valued Logic. In \textit{ACM Symposium on Principles of
        Programming Languages}. 2002.
    \item B. Steensgaard. Points-to Analysis in Almost Linear Time. In
        \textit{Proceedings of the 23rd ACM SIGPLAN-SIGACT symposium on
        Principles of Programming Languages}, pages 32-41, 1996.
    \item T. Tok, S. Guyer, and C. Lin. Efficient Flow-Sensitive
        Interprocedural Data-Flow Analysis in the Presence of Pointers. In
        \textit{LNCS, Springer-Verlag}, pages 17-31, 2006.
    \item X. Zhang, M. Naik, and H. Yang. Finding Optimum Abstractions in
        Parametric Dataflow Analysis. In \textit{Proceedings of the 34th ACM
        SIGPLAN conference on Programming Language Design and
        Implementation}, pages 365-375, 2013.
\end{enumerate}

\section*{Notes}

:: Survey Guidelines

1) Pick an area in which you are interested. Alternatively, pick a paper
   from a recent POPL or PLDI conference. 
2) Read thoroughly 3-6 papers (or a monograph?). Read at least superficially
   3-6 other papers. 
3) Write a report on what you have learned (max 6 pages)
        - What are the basic problems
        - What are the basic approaches to solving them
        - What are the main achievements to date
        - What are open problems
Keep the scope narrow enough so you can say something interesting, and cover
2-3 lectures worth of material.

===========================================================================

Topic: shape analysis.

:: What is the goal of shape analysis?

:: Why is this important?

Specific applications of shape analysis techniques include detecting null
pointer dereferences, faulty handling of memory deallocation or destruction,
and memory leaks.

:: Is shape analysis a hard problem? Why (what are the 'basic problems')?

Undecidibility, false positives, conservativeness, time bounds (?).

Loss of precision due to abstract interpretation, simplifications which do not
reflect the properties of real-world programs (e.g ignore NULL dereferences,
treat allocations as cons cells).

:: What are the general approaches to performing shape analysis?

- L\&H: Construct an `alias graph' using abstract interpretation over a lattice
  structure, then answer structural queries by traversing the graph.
- M\&S: Construct a `shape graph' via similar means. The meet operation of the
  shape graph lattice differs from that of L\&H, and indeed the structure of the
  shape nodes is also very different. This class of graph is still constructed
  via similar means (i.e abstract interpretation, with a lattice structure to
  merge graphs).

:: What is the current state-of-the-art in shape analysis?

- Candidate: shape graphs of M\&S, see their discussion.
- Candidate: a more recent paper by M\&S which concerns parameterized 3-valued
  logic.

- Mention demand-driven techniques (Atkinson, Griswold, 1998)

:: What unsolved problems remain? How can we make progress in this field?

- Make it faster
    - Select data structures with thought for the underlying architecture
    - Create parallel analyses
- Make it more precise
- Make it more usable: how do we implement practical, fast analyses?
	o Papers often omit how to implement a mechanism for submitting queries
	  to the `shape graph' or equivalent data structure.  
	o Detail on efficient, parallel computation of these structures is also
	  lacking.
\end{document}
